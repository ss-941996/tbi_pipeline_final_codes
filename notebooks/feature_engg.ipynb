{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dcafbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SIMPLE PIPELINE: TEST BEST MODEL ON RAW vs RAW+ENGINEERED FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# This takes your already-trained best model (XGBoost with tuned hyperparameters)\n",
    "# and evaluates it on two datasets:\n",
    "#   Dataset A: Raw features only\n",
    "#   Dataset B: Raw features + Engineered features\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd88690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1. FEATURE ENGINEERING CLASS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    Adds engineered features while keeping ALL original features.\n",
    "    Fit on training data, transform on both train and test.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.median_values = {}\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Learn parameters from training data only\"\"\"\n",
    "        X_clean = X.copy()\n",
    "        X_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        self.median_values = X_clean.median().to_dict()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Add engineered features to original dataset\"\"\"\n",
    "        X_eng = X.copy()\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # FIM Composites\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        comm_cols = ['FIMCompA', 'FIMExpressA']\n",
    "        if all(col in X_eng.columns for col in comm_cols):\n",
    "            X_eng['FIM_Communication_Score'] = X_eng[comm_cols].mean(axis=1)\n",
    "        \n",
    "        social_cols = ['FIMSocialA', 'FIMProbSlvA']\n",
    "        if all(col in X_eng.columns for col in social_cols):\n",
    "            X_eng['FIM_Social_Cognition'] = X_eng[social_cols].mean(axis=1)\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Efficiency ratios (avoid division by zero)\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if 'LOSRehabNoInt' in X_eng.columns:\n",
    "            los_safe = X_eng['LOSRehabNoInt'] + 1\n",
    "            \n",
    "            if 'FIMMOTA' in X_eng.columns:\n",
    "                X_eng['FIM_Motor_Efficiency'] = X_eng['FIMMOTA'] / los_safe\n",
    "            \n",
    "            if 'FIMCOGA' in X_eng.columns:\n",
    "                X_eng['FIM_Cognitive_Efficiency'] = X_eng['FIMCOGA'] / los_safe\n",
    "            \n",
    "            if 'FIM_Communication_Score' in X_eng.columns:\n",
    "                X_eng['Communication_per_Day'] = X_eng['FIM_Communication_Score'] / los_safe\n",
    "            \n",
    "            if 'FIM_Social_Cognition' in X_eng.columns:\n",
    "                X_eng['SocialCog_per_Day'] = X_eng['FIM_Social_Cognition'] / los_safe\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Cognitive-Motor interactions\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if 'FIMCOGA' in X_eng.columns and 'FIMMOTA' in X_eng.columns:\n",
    "            X_eng['FIM_CogMot_Ratio'] = X_eng['FIMCOGA'] / (X_eng['FIMMOTA'] + 1)\n",
    "            X_eng['FIM_CogMot_Product'] = X_eng['FIMCOGA'] * X_eng['FIMMOTA']\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Cognitive items average\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        fim_cog_items = ['FIMCompA', 'FIMExpressA', 'FIMProbSlvA', 'FIMSocialA']\n",
    "        if all(col in X_eng.columns for col in fim_cog_items):\n",
    "            X_eng['FIM_CogItems_Avg'] = X_eng[fim_cog_items].mean(axis=1)\n",
    "        \n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Clean data\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        X_eng.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        X_eng.fillna(self.median_values, inplace=True)\n",
    "        X_eng.columns = X_eng.columns.str.replace('[<>[\\],]', '_', regex=True)\n",
    "        \n",
    "        return X_eng\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "\n",
    "def prepare_raw_features(X, median_values=None):\n",
    "    \"\"\"Clean raw features without adding new ones\"\"\"\n",
    "    X_raw = X.copy()\n",
    "    X_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    if median_values is None:\n",
    "        X_raw.fillna(X_raw.median(), inplace=True)\n",
    "    else:\n",
    "        X_raw.fillna(median_values, inplace=True)\n",
    "    \n",
    "    X_raw.columns = X_raw.columns.str.replace('[<>[\\],]', '_', regex=True)\n",
    "    return X_raw\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2. EVALUATION FUNCTIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate all regression metrics\"\"\"\n",
    "    return {\n",
    "        'r2': r2_score(y_true, y_pred),\n",
    "        'mae': mean_absolute_error(y_true, y_pred),\n",
    "        'rmse': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'mse': mean_squared_error(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_comparison(y_true, y_pred_raw, y_pred_eng, n_bootstrap=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Bootstrap evaluation to compare two sets of predictions.\n",
    "    Returns metrics for both models and statistical tests.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    n = len(y_true)\n",
    "    \n",
    "    metrics_raw = {'r2': [], 'mae': [], 'rmse': []}\n",
    "    metrics_eng = {'r2': [], 'mae': [], 'rmse': []}\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "        y_true_bs = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "        y_pred_raw_bs = y_pred_raw[indices]\n",
    "        y_pred_eng_bs = y_pred_eng[indices]\n",
    "        \n",
    "        # Raw model metrics\n",
    "        metrics_raw['r2'].append(r2_score(y_true_bs, y_pred_raw_bs))\n",
    "        metrics_raw['mae'].append(mean_absolute_error(y_true_bs, y_pred_raw_bs))\n",
    "        metrics_raw['rmse'].append(np.sqrt(mean_squared_error(y_true_bs, y_pred_raw_bs)))\n",
    "        \n",
    "        # Engineered model metrics\n",
    "        metrics_eng['r2'].append(r2_score(y_true_bs, y_pred_eng_bs))\n",
    "        metrics_eng['mae'].append(mean_absolute_error(y_true_bs, y_pred_eng_bs))\n",
    "        metrics_eng['rmse'].append(np.sqrt(mean_squared_error(y_true_bs, y_pred_eng_bs)))\n",
    "    \n",
    "    # Calculate confidence intervals and statistics\n",
    "    results = {}\n",
    "    for metric in ['r2', 'mae', 'rmse']:\n",
    "        raw_vals = np.array(metrics_raw[metric])\n",
    "        eng_vals = np.array(metrics_eng[metric])\n",
    "        diff_vals = eng_vals - raw_vals\n",
    "        \n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(eng_vals, raw_vals)\n",
    "        \n",
    "        results[metric] = {\n",
    "            'raw_mean': np.mean(raw_vals),\n",
    "            'raw_ci': np.percentile(raw_vals, [2.5, 97.5]),\n",
    "            'eng_mean': np.mean(eng_vals),\n",
    "            'eng_ci': np.percentile(eng_vals, [2.5, 97.5]),\n",
    "            'diff_mean': np.mean(diff_vals),\n",
    "            'diff_ci': np.percentile(diff_vals, [2.5, 97.5]),\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3. MAIN COMPARISON FUNCTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def compare_raw_vs_engineered(\n",
    "    filepath,\n",
    "    best_model_pipeline,\n",
    "    target_col='FIM_change',\n",
    "    test_size=0.2,\n",
    "    n_bootstrap=1000,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare best model's performance on:\n",
    "    - Dataset A: Raw features only\n",
    "    - Dataset B: Raw features + Engineered features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        Path to your data CSV\n",
    "    best_model_pipeline : Pipeline or model\n",
    "         already-trained best model (e.g., from final_models_for_comparison['XGBRegressor'])\n",
    "    target_col : str\n",
    "        Target variable name\n",
    "    test_size : float\n",
    "        Proportion for test set\n",
    "    n_bootstrap : int\n",
    "        Number of bootstrap iterations\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : Contains all comparison results, models, and predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"FEATURE ENGINEERING COMPARISON: RAW vs RAW+ENGINEERED\")\n",
    "    print(\"Using your best trained model from the original pipeline\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 1: Load and split data\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸ“Š Step 1: Loading and splitting data...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    X = df.drop(columns=['Mod1Id', target_col] if 'Mod1Id' in df.columns else [target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split data BEFORE feature engineering\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"   Training set: {len(X_train)} samples\")\n",
    "    print(f\"   Test set: {len(X_test)} samples\")\n",
    "    print(f\"   Original features: {X_train.shape[1]} columns\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 2: Prepare Dataset A (RAW features only)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸ”§ Step 2: Preparing Dataset A (Raw features only)...\")\n",
    "    \n",
    "    # Clean training data and get medians\n",
    "    X_train_raw_temp = X_train.copy()\n",
    "    X_train_raw_temp.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    raw_medians = X_train_raw_temp.median().to_dict()\n",
    "    \n",
    "    # Apply to train and test\n",
    "    X_train_raw = prepare_raw_features(X_train, raw_medians)\n",
    "    X_test_raw = prepare_raw_features(X_test, raw_medians)\n",
    "    \n",
    "    print(f\"   Dataset A features: {X_train_raw.shape[1]} columns\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 3: Prepare Dataset B (RAW + ENGINEERED)\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nâš™ï¸  Step 3: Preparing Dataset B (Raw + Engineered features)...\")\n",
    "    \n",
    "    fe = FeatureEngineer()\n",
    "    X_train_eng = fe.fit_transform(X_train)\n",
    "    X_test_eng = fe.transform(X_test)\n",
    "    \n",
    "    new_features = [col for col in X_train_eng.columns if col not in X_train_raw.columns]\n",
    "    \n",
    "    print(f\"   Dataset B features: {X_train_eng.shape[1]} columns\")\n",
    "    print(f\"   New features added: {len(new_features)}\")\n",
    "    print(f\"\\n   ğŸ“ Engineered features:\")\n",
    "    for feat in new_features:\n",
    "        print(f\"      â€¢ {feat}\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 4: Clone and retrain model on both datasets\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸ¤– Step 4: Training models on both datasets...\")\n",
    "    print(\"   (Cloning your best model with same hyperparameters)\")\n",
    "    \n",
    "    # Clone the model to ensure independent training\n",
    "    model_raw = clone(best_model_pipeline)\n",
    "    model_eng = clone(best_model_pipeline)\n",
    "    \n",
    "    print(\"   Training Model A on Dataset A (Raw features)...\")\n",
    "    model_raw.fit(X_train_raw, y_train)\n",
    "    \n",
    "    print(\"   Training Model B on Dataset B (Raw + Engineered features)...\")\n",
    "    model_eng.fit(X_train_eng, y_train)\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 5: Predict on test set\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\nğŸ¯ Step 5: Evaluating on test set...\")\n",
    "    \n",
    "    y_pred_raw = model_raw.predict(X_test_raw)\n",
    "    y_pred_eng = model_eng.predict(X_test_eng)\n",
    "    \n",
    "    metrics_raw = calculate_metrics(y_test, y_pred_raw)\n",
    "    metrics_eng = calculate_metrics(y_test, y_pred_eng)\n",
    "    \n",
    "    print(\"\\n   ğŸ“Š Test Set Results:\")\n",
    "    print(\"   \" + \"-\" * 75)\n",
    "    print(f\"   {'Metric':<15} {'Model A (Raw)':>18} {'Model B (Raw+Eng)':>18} {'Î”':>10} {'Better'}\")\n",
    "    print(\"   \" + \"-\" * 75)\n",
    "    \n",
    "    for metric in ['r2', 'mae', 'rmse', 'mse']:\n",
    "        raw_val = metrics_raw[metric]\n",
    "        eng_val = metrics_eng[metric]\n",
    "        diff = eng_val - raw_val\n",
    "        \n",
    "        # Determine which is better\n",
    "        if metric == 'r2':\n",
    "            better = \"B âœ“\" if diff > 0 else \"A âœ“\"\n",
    "        else:  # Lower is better for error metrics\n",
    "            better = \"B âœ“\" if diff < 0 else \"A âœ“\"\n",
    "        \n",
    "        print(f\"   {metric.upper():<15} {raw_val:>18.4f} {eng_val:>18.4f} {diff:>9.4f}  {better}\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 6: Bootstrap validation with statistical testing\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(f\"\\nğŸ”¬ Step 6: Bootstrap validation ({n_bootstrap} iterations)...\")\n",
    "    print(\"   This may take a moment...\")\n",
    "    \n",
    "    bootstrap_results = bootstrap_comparison(\n",
    "        y_test, y_pred_raw, y_pred_eng, \n",
    "        n_bootstrap=n_bootstrap, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(\"\\n   ğŸ“‰ Bootstrap Results with 95% Confidence Intervals:\")\n",
    "    print(\"   \" + \"-\" * 80)\n",
    "    \n",
    "    for metric in ['r2', 'mae', 'rmse']:\n",
    "        res = bootstrap_results[metric]\n",
    "        print(f\"\\n   {metric.upper()}:\")\n",
    "        print(f\"      Model A (Raw):          {res['raw_mean']:>7.4f}  \"\n",
    "              f\"(95% CI: [{res['raw_ci'][0]:>7.4f}, {res['raw_ci'][1]:>7.4f}])\")\n",
    "        print(f\"      Model B (Raw+Eng):      {res['eng_mean']:>7.4f}  \"\n",
    "              f\"(95% CI: [{res['eng_ci'][0]:>7.4f}, {res['eng_ci'][1]:>7.4f}])\")\n",
    "        print(f\"      Difference (B - A):     {res['diff_mean']:>7.4f}  \"\n",
    "              f\"(95% CI: [{res['diff_ci'][0]:>7.4f}, {res['diff_ci'][1]:>7.4f}])\")\n",
    "    \n",
    "    print(\"\\n   ğŸ” Statistical Significance Tests (Paired t-tests):\")\n",
    "    print(\"   \" + \"-\" * 80)\n",
    "    print(f\"   {'Metric':<8} {'Mean Î”':>10} {'t-statistic':>12} {'p-value':>10} {'Significant'}\")\n",
    "    print(\"   \" + \"-\" * 80)\n",
    "    \n",
    "    for metric in ['r2', 'mae', 'rmse']:\n",
    "        res = bootstrap_results[metric]\n",
    "        sig_marker = \"Yes ***\" if res['significant'] else \"No\"\n",
    "        print(f\"   {metric.upper():<8} {res['diff_mean']:>10.4f} {res['t_statistic']:>12.3f} \"\n",
    "              f\"{res['p_value']:>10.4f}   {sig_marker}\")\n",
    "    \n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    # Step 7: Final recommendation\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“‹ SUMMARY AND RECOMMENDATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    r2_res = bootstrap_results['r2']\n",
    "    mae_res = bootstrap_results['mae']\n",
    "    \n",
    "    print(f\"\\n   Performance Differences (Model B - Model A):\")\n",
    "    print(f\"   â€¢ RÂ² change:    {r2_res['diff_mean']:+.4f} (p = {r2_res['p_value']:.4f})\")\n",
    "    print(f\"   â€¢ MAE change:   {mae_res['diff_mean']:+.4f} (p = {mae_res['p_value']:.4f})\")\n",
    "    \n",
    "    if r2_res['diff_mean'] > 0 and r2_res['significant']:\n",
    "        print(\"\\n   âœ… MODEL B (Raw + Engineered Features) is SIGNIFICANTLY BETTER\")\n",
    "        print(f\"      RÂ² improved by {r2_res['diff_mean']:.4f} (95% CI: [{r2_res['diff_ci'][0]:.4f}, {r2_res['diff_ci'][1]:.4f}])\")\n",
    "        if mae_res['diff_mean'] < 0:\n",
    "            print(f\"      MAE reduced by {abs(mae_res['diff_mean']):.4f}\")\n",
    "        print(\"\\n   ğŸ’¡ Recommendation: Use Dataset B with engineered features\")\n",
    "        print(f\"      The {len(new_features)} engineered features provide significant value\")\n",
    "        \n",
    "    elif r2_res['diff_mean'] < 0 and r2_res['significant']:\n",
    "        print(\"\\n   âš ï¸  MODEL A (Raw Features Only) is SIGNIFICANTLY BETTER\")\n",
    "        print(f\"      RÂ² decreased by {abs(r2_res['diff_mean']):.4f} when adding features\")\n",
    "        print(\"\\n   ğŸ’¡ Recommendation: Use Dataset A (raw features only)\")\n",
    "        print(\"      Engineered features may be adding noise or causing overfitting\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n   â– NO SIGNIFICANT DIFFERENCE between models\")\n",
    "        print(f\"      RÂ² difference: {r2_res['diff_mean']:.4f} (p = {r2_res['p_value']:.4f})\")\n",
    "        print(\"\\n   ğŸ’¡ Recommendation: Use Dataset A (raw features) - Occam's Razor\")\n",
    "        print(\"      When performance is similar, prefer the simpler model\")\n",
    "    \n",
    "    # Check if CI crosses zero\n",
    "    if r2_res['diff_ci'][0] < 0 < r2_res['diff_ci'][1]:\n",
    "        print(\"\\n   âš ï¸  Note: 95% CI for RÂ² difference includes zero\")\n",
    "        print(\"      This suggests high uncertainty in the performance difference\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    return {\n",
    "        'test_metrics': {\n",
    "            'raw': metrics_raw,\n",
    "            'engineered': metrics_eng\n",
    "        },\n",
    "        'bootstrap_results': bootstrap_results,\n",
    "        'predictions': {\n",
    "            'y_test': y_test,\n",
    "            'y_pred_raw': y_pred_raw,\n",
    "            'y_pred_eng': y_pred_eng\n",
    "        },\n",
    "        'models': {\n",
    "            'model_raw': model_raw,\n",
    "            'model_eng': model_eng\n",
    "        },\n",
    "        'datasets': {\n",
    "            'X_train_raw': X_train_raw,\n",
    "            'X_train_eng': X_train_eng,\n",
    "            'X_test_raw': X_test_raw,\n",
    "            'X_test_eng': X_test_eng\n",
    "        },\n",
    "        'new_features': new_features\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd473b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded successfully!\n",
      "================================================================================\n",
      "FEATURE ENGINEERING COMPARISON: RAW vs RAW+ENGINEERED\n",
      "Using your best trained model from the original pipeline\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Step 1: Loading and splitting data...\n",
      "   Training set: 10963 samples\n",
      "   Test set: 2741 samples\n",
      "   Original features: 141 columns\n",
      "\n",
      "ğŸ”§ Step 2: Preparing Dataset A (Raw features only)...\n",
      "   Dataset A features: 141 columns\n",
      "\n",
      "âš™ï¸  Step 3: Preparing Dataset B (Raw + Engineered features)...\n",
      "   Dataset B features: 150 columns\n",
      "   New features added: 9\n",
      "\n",
      "   ğŸ“ Engineered features:\n",
      "      â€¢ FIM_Communication_Score\n",
      "      â€¢ FIM_Social_Cognition\n",
      "      â€¢ FIM_Motor_Efficiency\n",
      "      â€¢ FIM_Cognitive_Efficiency\n",
      "      â€¢ Communication_per_Day\n",
      "      â€¢ SocialCog_per_Day\n",
      "      â€¢ FIM_CogMot_Ratio\n",
      "      â€¢ FIM_CogMot_Product\n",
      "      â€¢ FIM_CogItems_Avg\n",
      "\n",
      "ğŸ¤– Step 4: Training models on both datasets...\n",
      "   (Cloning your best model with same hyperparameters)\n",
      "   Training Model A on Dataset A (Raw features)...\n",
      "   Training Model B on Dataset B (Raw + Engineered features)...\n",
      "\n",
      "ğŸ¯ Step 5: Evaluating on test set...\n",
      "\n",
      "   ğŸ“Š Test Set Results:\n",
      "   ---------------------------------------------------------------------------\n",
      "   Metric               Model A (Raw)  Model B (Raw+Eng)          Î” Better\n",
      "   ---------------------------------------------------------------------------\n",
      "   R2                          0.3105             0.3104   -0.0001  A âœ“\n",
      "   MAE                        10.6581            10.6694    0.0113  A âœ“\n",
      "   RMSE                       14.7266            14.7275    0.0010  A âœ“\n",
      "   MSE                       216.8721           216.9007    0.0286  A âœ“\n",
      "\n",
      "ğŸ”¬ Step 6: Bootstrap validation (1000 iterations)...\n",
      "   This may take a moment...\n",
      "\n",
      "   ğŸ“‰ Bootstrap Results with 95% Confidence Intervals:\n",
      "   --------------------------------------------------------------------------------\n",
      "\n",
      "   R2:\n",
      "      Model A (Raw):           0.3106  (95% CI: [ 0.2787,  0.3428])\n",
      "      Model B (Raw+Eng):       0.3106  (95% CI: [ 0.2781,  0.3425])\n",
      "      Difference (B - A):     -0.0001  (95% CI: [-0.0061,  0.0060])\n",
      "\n",
      "   MAE:\n",
      "      Model A (Raw):          10.6522  (95% CI: [10.3023, 11.0476])\n",
      "      Model B (Raw+Eng):      10.6632  (95% CI: [10.2935, 11.0547])\n",
      "      Difference (B - A):      0.0109  (95% CI: [-0.0413,  0.0618])\n",
      "\n",
      "   RMSE:\n",
      "      Model A (Raw):          14.7086  (95% CI: [14.0765, 15.3165])\n",
      "      Model B (Raw+Eng):      14.7096  (95% CI: [14.0943, 15.3005])\n",
      "      Difference (B - A):      0.0010  (95% CI: [-0.0638,  0.0652])\n",
      "\n",
      "   ğŸ” Statistical Significance Tests (Paired t-tests):\n",
      "   --------------------------------------------------------------------------------\n",
      "   Metric       Mean Î”  t-statistic    p-value Significant\n",
      "   --------------------------------------------------------------------------------\n",
      "   R2          -0.0001       -0.945     0.3447   No\n",
      "   MAE          0.0109       13.075     0.0000   Yes ***\n",
      "   RMSE         0.0010        0.948     0.3435   No\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‹ SUMMARY AND RECOMMENDATION\n",
      "================================================================================\n",
      "\n",
      "   Performance Differences (Model B - Model A):\n",
      "   â€¢ RÂ² change:    -0.0001 (p = 0.3447)\n",
      "   â€¢ MAE change:   +0.0109 (p = 0.0000)\n",
      "\n",
      "   â– NO SIGNIFICANT DIFFERENCE between models\n",
      "      RÂ² difference: -0.0001 (p = 0.3447)\n",
      "\n",
      "   ğŸ’¡ Recommendation: Use Dataset A (raw features) - Occam's Razor\n",
      "      When performance is similar, prefer the simpler model\n",
      "\n",
      "   âš ï¸  Note: 95% CI for RÂ² difference includes zero\n",
      "      This suggests high uncertainty in the performance difference\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Final Comparison Results:\n",
      "{'test_metrics': {'raw': {'r2': 0.3104516267776489, 'mae': 10.6581392288208, 'rmse': 14.726578042100552, 'mse': 216.87210083007812}, 'engineered': {'r2': 0.31036078929901123, 'mae': 10.66943645477295, 'rmse': 14.727548354766729, 'mse': 216.9006805419922}}, 'bootstrap_results': {'r2': {'raw_mean': 0.310644778072834, 'raw_ci': array([0.27867188, 0.34277662]), 'eng_mean': 0.31055359679460526, 'eng_ci': array([0.27814304, 0.34245571]), 'diff_mean': -9.118127822875977e-05, 'diff_ci': array([-0.00613162,  0.00598594]), 't_statistic': -0.9453015595513855, 'p_value': 0.34473351596579016, 'significant': False}, 'mae': {'raw_mean': 10.652244811058045, 'raw_ci': array([10.302303  , 11.04757795]), 'eng_mean': 10.663175750732423, 'eng_ci': array([10.29346738, 11.05470042]), 'diff_mean': 0.010930939674377442, 'diff_ci': array([-0.04129338,  0.06175785]), 't_statistic': 13.074673249382702, 'p_value': 3.557433677741678e-36, 'significant': True}, 'rmse': {'raw_mean': 14.708620099265328, 'raw_ci': array([14.07652526, 15.31647305]), 'eng_mean': 14.709595036350333, 'eng_ci': array([14.09425009, 15.30050934]), 'diff_mean': 0.0009749370850049032, 'diff_ci': array([-0.06376352,  0.06517207]), 't_statistic': 0.947711022373813, 'p_value': 0.34350577279402184, 'significant': False}}, 'predictions': {'y_test': 7586     37\n",
      "4974     26\n",
      "4013     36\n",
      "5860     13\n",
      "2117     22\n",
      "         ..\n",
      "43       31\n",
      "17       61\n",
      "12818    20\n",
      "8341    -33\n",
      "1883     24\n",
      "Name: FIM_change, Length: 2741, dtype: int64, 'y_pred_raw': array([13.545251 , 16.503092 , 21.297922 , ..., 21.607958 , 10.0213175,\n",
      "       27.169575 ], dtype=float32), 'y_pred_eng': array([13.791169, 18.098675, 22.845566, ..., 22.841255, 10.367126,\n",
      "       26.942339], dtype=float32)}, 'models': {'model_raw': Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('xgb',\n",
      "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "                              colsample_bylevel=None, colsample_bynode=None,\n",
      "                              colsample_bytree=0.7760609974958406, device=None,\n",
      "                              early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              feature_types=None, feature_weights=None,\n",
      "                              gamma=None, grow_policy=None,\n",
      "                              importance_type=None,\n",
      "                              interaction_constraints=None,\n",
      "                              learning_rate=0.013399060561509796, max_bin=None,\n",
      "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "                              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, multi_strategy=None,\n",
      "                              n_estimators=666, n_jobs=-1,\n",
      "                              num_parallel_tree=None, ...))]), 'model_eng': Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
      "                ('xgb',\n",
      "                 XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "                              colsample_bylevel=None, colsample_bynode=None,\n",
      "                              colsample_bytree=0.7760609974958406, device=None,\n",
      "                              early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              feature_types=None, feature_weights=None,\n",
      "                              gamma=None, grow_policy=None,\n",
      "                              importance_type=None,\n",
      "                              interaction_constraints=None,\n",
      "                              learning_rate=0.013399060561509796, max_bin=None,\n",
      "                              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "                              max_delta_step=None, max_depth=6, max_leaves=None,\n",
      "                              min_child_weight=None, missing=nan,\n",
      "                              monotone_constraints=None, multi_strategy=None,\n",
      "                              n_estimators=666, n_jobs=-1,\n",
      "                              num_parallel_tree=None, ...))])}, 'datasets': {'X_train_raw':        SexF  AcutePay1  Cause  SCI  GCSEye  GCSVer  GCSMot  GCSTot  RTSResp  \\\n",
      "4872    2.0        2.0   19.0  1.0     3.0     5.0     6.0    14.0     32.0   \n",
      "9902    2.0        4.0   19.0  0.0     1.0     1.0     5.0     7.0     26.0   \n",
      "5589    1.0        4.0   19.0  0.0     4.0     5.0     6.0    15.0     18.0   \n",
      "11441   2.0        4.0    2.0  0.0     7.0     8.0     7.0    13.0     14.0   \n",
      "11587   2.0       15.0    1.0  0.0     1.0     8.0     1.0    13.0     12.0   \n",
      "...     ...        ...    ...  ...     ...     ...     ...     ...      ...   \n",
      "5191    2.0       14.0    1.0  0.0     4.0     4.0     6.0    14.0     22.0   \n",
      "13418   1.0        1.0    3.0  0.0     4.0     5.0     6.0    15.0     18.0   \n",
      "5390    2.0        4.0    3.0  0.0     3.0     4.0     5.0    12.0     18.0   \n",
      "860     2.0        4.0    1.0  0.0     2.0     4.0     4.0    10.0     18.0   \n",
      "7270    2.0        3.0   19.0  0.0     4.0     5.0     6.0    15.0     18.0   \n",
      "\n",
      "       RTSBP  ...  INJYEAR  LOSAcute  LOSRehab  LOSRehabNoInt  LOSTot  \\\n",
      "4872   119.0  ...     2018      24.0      27.0           27.0    51.0   \n",
      "9902   129.0  ...     2007      27.0      52.0           48.0    75.0   \n",
      "5589   139.0  ...     2014       9.0      30.0           30.0    39.0   \n",
      "11441  126.0  ...     2009      23.0      28.0           28.0    51.0   \n",
      "11587  151.0  ...     2008      61.0      57.0           57.0   118.0   \n",
      "...      ...  ...      ...       ...       ...            ...     ...   \n",
      "5191   135.0  ...     2011      10.0       6.0            6.0    16.0   \n",
      "13418  131.0  ...     2019       2.0       0.0            0.0     2.0   \n",
      "5390   146.0  ...     2004      17.0      25.0           25.0    42.0   \n",
      "860    139.0  ...     2013       5.0      14.0           14.0    19.0   \n",
      "7270   139.0  ...     2013      21.0      37.0           37.0    58.0   \n",
      "\n",
      "       PROBLEMUse  PTADays  RURALadm  RURALdc  TFCDays  \n",
      "4872          1.0      1.0       2.0      2.0      1.0  \n",
      "9902          1.0     18.0       2.0      2.0     13.0  \n",
      "5589          0.0      0.0       2.0      2.0      0.5  \n",
      "11441         0.0     30.0       3.0      3.0      1.0  \n",
      "11587         1.0     18.0       2.0      2.0     54.0  \n",
      "...           ...      ...       ...      ...      ...  \n",
      "5191          1.0     14.0       2.0      2.0      0.5  \n",
      "13418         1.0     18.0       3.0      3.0      1.0  \n",
      "5390          1.0     15.0       2.0      3.0     15.0  \n",
      "860           0.0      4.0       2.0      2.0      0.5  \n",
      "7270          1.0     39.0       2.0      2.0      0.5  \n",
      "\n",
      "[10963 rows x 141 columns], 'X_train_eng':        SexF  AcutePay1  Cause  SCI  GCSEye  GCSVer  GCSMot  GCSTot  RTSResp  \\\n",
      "4872    2.0        2.0   19.0  1.0     3.0     5.0     6.0    14.0     32.0   \n",
      "9902    2.0        4.0   19.0  0.0     1.0     1.0     5.0     7.0     26.0   \n",
      "5589    1.0        4.0   19.0  0.0     4.0     5.0     6.0    15.0     18.0   \n",
      "11441   2.0        4.0    2.0  0.0     7.0     8.0     7.0    13.0     14.0   \n",
      "11587   2.0       15.0    1.0  0.0     1.0     8.0     1.0    13.0     12.0   \n",
      "...     ...        ...    ...  ...     ...     ...     ...     ...      ...   \n",
      "5191    2.0       14.0    1.0  0.0     4.0     4.0     6.0    14.0     22.0   \n",
      "13418   1.0        1.0    3.0  0.0     4.0     5.0     6.0    15.0     18.0   \n",
      "5390    2.0        4.0    3.0  0.0     3.0     4.0     5.0    12.0     18.0   \n",
      "860     2.0        4.0    1.0  0.0     2.0     4.0     4.0    10.0     18.0   \n",
      "7270    2.0        3.0   19.0  0.0     4.0     5.0     6.0    15.0     18.0   \n",
      "\n",
      "       RTSBP  ...  TFCDays  FIM_Communication_Score  FIM_Social_Cognition  \\\n",
      "4872   119.0  ...      1.0                      6.0                   5.0   \n",
      "9902   129.0  ...     13.0                      1.0                   1.5   \n",
      "5589   139.0  ...      0.5                      2.5                   1.5   \n",
      "11441  126.0  ...      1.0                      1.5                   2.0   \n",
      "11587  151.0  ...     54.0                      1.0                   1.0   \n",
      "...      ...  ...      ...                      ...                   ...   \n",
      "5191   135.0  ...      0.5                      4.5                   2.5   \n",
      "13418  131.0  ...      1.0                      4.0                   3.5   \n",
      "5390   146.0  ...     15.0                      1.0                   1.0   \n",
      "860    139.0  ...      0.5                      4.0                   3.0   \n",
      "7270   139.0  ...      0.5                      1.0                   1.0   \n",
      "\n",
      "       FIM_Motor_Efficiency  FIM_Cognitive_Efficiency  Communication_per_Day  \\\n",
      "4872               0.892857                  1.000000               0.214286   \n",
      "9902               0.551020                  0.142857               0.020408   \n",
      "5589               1.096774                  0.322581               0.080645   \n",
      "11441              0.482759                  0.275862               0.051724   \n",
      "11587              0.293103                  0.086207               0.017241   \n",
      "...                     ...                       ...                    ...   \n",
      "5191               5.142857                  2.428571               0.642857   \n",
      "13418             49.000000                 19.000000               4.000000   \n",
      "5390               0.692308                  0.192308               0.038462   \n",
      "860                3.466667                  1.133333               0.266667   \n",
      "7270               0.473684                  0.131579               0.026316   \n",
      "\n",
      "       SocialCog_per_Day  FIM_CogMot_Ratio  FIM_CogMot_Product  \\\n",
      "4872            0.178571          1.076923               700.0   \n",
      "9902            0.030612          0.250000               189.0   \n",
      "5589            0.048387          0.285714               340.0   \n",
      "11441           0.068966          0.533333               112.0   \n",
      "11587           0.017241          0.277778                85.0   \n",
      "...                  ...               ...                 ...   \n",
      "5191            0.357143          0.459459               612.0   \n",
      "13418           3.500000          0.380000               931.0   \n",
      "5390            0.038462          0.263158                90.0   \n",
      "860             0.200000          0.320755               884.0   \n",
      "7270            0.026316          0.263158                90.0   \n",
      "\n",
      "       FIM_CogItems_Avg  \n",
      "4872               5.50  \n",
      "9902               1.25  \n",
      "5589               2.00  \n",
      "11441              1.75  \n",
      "11587              1.00  \n",
      "...                 ...  \n",
      "5191               3.50  \n",
      "13418              3.75  \n",
      "5390               1.00  \n",
      "860                3.50  \n",
      "7270               1.00  \n",
      "\n",
      "[10963 rows x 150 columns], 'X_test_raw':        SexF  AcutePay1  Cause  SCI  GCSEye  GCSVer  GCSMot  GCSTot  RTSResp  \\\n",
      "7586    2.0        1.0   19.0  0.0     4.0     5.0     6.0    13.0     18.0   \n",
      "4974    1.0        2.0   21.0  1.0     4.0     5.0     6.0    15.0     24.0   \n",
      "4013    1.0        4.0   10.0  0.0     4.0     5.0     6.0    15.0     23.0   \n",
      "5860    2.0        4.0    1.0  0.0     7.0     7.0     7.0    13.0     20.0   \n",
      "2117    1.0        4.0    1.0  0.0     4.0     4.0     6.0    14.0     22.0   \n",
      "...     ...        ...    ...  ...     ...     ...     ...     ...      ...   \n",
      "43      2.0        4.0   19.0  0.0     4.0     5.0     6.0    15.0     20.0   \n",
      "17      2.0       14.0   12.0  0.0     1.0     8.0     1.0    13.0     23.0   \n",
      "12818   2.0        4.0    1.0  0.0     1.0     8.0     1.0    13.0     18.0   \n",
      "8341    2.0        4.0   20.0  0.0     4.0     2.0     1.0     7.0     20.0   \n",
      "1883    2.0       10.0    1.0  0.0     4.0     8.0     5.0    13.0     18.0   \n",
      "\n",
      "       RTSBP  ...  INJYEAR  LOSAcute  LOSRehab  LOSRehabNoInt  LOSTot  \\\n",
      "7586   139.0  ...     2014       6.0      13.0           13.0    19.0   \n",
      "4974    96.0  ...     2018      44.0      76.0           72.0   116.0   \n",
      "4013   112.0  ...     2010      16.0      21.0           21.0    37.0   \n",
      "5860   177.0  ...     2008      18.0      14.0           14.0    32.0   \n",
      "2117   104.0  ...     2001       9.0      10.0           10.0    19.0   \n",
      "...      ...  ...      ...       ...       ...            ...     ...   \n",
      "43     162.0  ...     2006       6.0       8.0            8.0    14.0   \n",
      "17      85.0  ...     2017      23.0      28.0           28.0    51.0   \n",
      "12818  139.0  ...     2016      15.0       8.0            8.0    23.0   \n",
      "8341   122.0  ...     2018       9.0      16.0           16.0    25.0   \n",
      "1883   139.0  ...     2015      13.0       6.0            6.0    19.0   \n",
      "\n",
      "       PROBLEMUse  PTADays  RURALadm  RURALdc  TFCDays  \n",
      "7586          1.0     11.0       2.0      2.0      2.0  \n",
      "4974          0.0     45.0       2.0      2.0      0.5  \n",
      "4013          0.0     26.0       1.0      1.0      1.0  \n",
      "5860          0.0     18.0       1.0      1.0      7.0  \n",
      "2117          0.0      4.0       3.0      3.0      2.0  \n",
      "...           ...      ...       ...      ...      ...  \n",
      "43            0.0      0.0       2.0      2.0      0.5  \n",
      "17            1.0     18.0       2.0      3.0      8.0  \n",
      "12818         0.0     18.0       1.0      3.0      5.0  \n",
      "8341          1.0      3.0       2.0      2.0      0.5  \n",
      "1883          1.0      9.0       2.0      1.0      4.0  \n",
      "\n",
      "[2741 rows x 141 columns], 'X_test_eng':        SexF  AcutePay1  Cause  SCI  GCSEye  GCSVer  GCSMot  GCSTot  RTSResp  \\\n",
      "7586    2.0        1.0   19.0  0.0     4.0     5.0     6.0    13.0     18.0   \n",
      "4974    1.0        2.0   21.0  1.0     4.0     5.0     6.0    15.0     24.0   \n",
      "4013    1.0        4.0   10.0  0.0     4.0     5.0     6.0    15.0     23.0   \n",
      "5860    2.0        4.0    1.0  0.0     7.0     7.0     7.0    13.0     20.0   \n",
      "2117    1.0        4.0    1.0  0.0     4.0     4.0     6.0    14.0     22.0   \n",
      "...     ...        ...    ...  ...     ...     ...     ...     ...      ...   \n",
      "43      2.0        4.0   19.0  0.0     4.0     5.0     6.0    15.0     20.0   \n",
      "17      2.0       14.0   12.0  0.0     1.0     8.0     1.0    13.0     23.0   \n",
      "12818   2.0        4.0    1.0  0.0     1.0     8.0     1.0    13.0     18.0   \n",
      "8341    2.0        4.0   20.0  0.0     4.0     2.0     1.0     7.0     20.0   \n",
      "1883    2.0       10.0    1.0  0.0     4.0     8.0     5.0    13.0     18.0   \n",
      "\n",
      "       RTSBP  ...  TFCDays  FIM_Communication_Score  FIM_Social_Cognition  \\\n",
      "7586   139.0  ...      2.0                      5.0                   5.0   \n",
      "4974    96.0  ...      0.5                      3.5                   2.5   \n",
      "4013   112.0  ...      1.0                      5.0                   4.5   \n",
      "5860   177.0  ...      7.0                      5.0                   4.5   \n",
      "2117   104.0  ...      2.0                      7.0                   5.0   \n",
      "...      ...  ...      ...                      ...                   ...   \n",
      "43     162.0  ...      0.5                      3.0                   2.0   \n",
      "17      85.0  ...      8.0                      1.0                   1.0   \n",
      "12818  139.0  ...      5.0                      5.0                   3.5   \n",
      "8341   122.0  ...      0.5                      2.0                   2.0   \n",
      "1883   139.0  ...      4.0                      6.0                   5.5   \n",
      "\n",
      "       FIM_Motor_Efficiency  FIM_Cognitive_Efficiency  Communication_per_Day  \\\n",
      "7586               3.571429                  1.714286               0.357143   \n",
      "4974               0.205479                  0.191781               0.047945   \n",
      "4013               1.954545                  0.954545               0.227273   \n",
      "5860               2.266667                  1.533333               0.333333   \n",
      "2117               5.000000                  2.636364               0.636364   \n",
      "...                     ...                       ...                    ...   \n",
      "43                 6.888889                  1.333333               0.333333   \n",
      "17                 0.448276                  0.172414               0.034483   \n",
      "12818              5.777778                  2.333333               0.555556   \n",
      "8341               1.470588                  0.588235               0.117647   \n",
      "1883               6.000000                  4.142857               0.857143   \n",
      "\n",
      "       SocialCog_per_Day  FIM_CogMot_Ratio  FIM_CogMot_Product  \\\n",
      "7586            0.357143          0.470588              1200.0   \n",
      "4974            0.034247          0.875000               210.0   \n",
      "4013            0.204545          0.477273               903.0   \n",
      "5860            0.300000          0.657143               782.0   \n",
      "2117            0.454545          0.517857              1595.0   \n",
      "...                  ...               ...                 ...   \n",
      "43              0.222222          0.190476               744.0   \n",
      "17              0.034483          0.357143                65.0   \n",
      "12818           0.388889          0.396226              1092.0   \n",
      "8341            0.117647          0.384615               250.0   \n",
      "1883            0.785714          0.674419              1218.0   \n",
      "\n",
      "       FIM_CogItems_Avg  \n",
      "7586               5.00  \n",
      "4974               3.00  \n",
      "4013               4.75  \n",
      "5860               4.75  \n",
      "2117               6.00  \n",
      "...                 ...  \n",
      "43                 2.50  \n",
      "17                 1.00  \n",
      "12818              4.25  \n",
      "8341               2.00  \n",
      "1883               5.75  \n",
      "\n",
      "[2741 rows x 150 columns]}, 'new_features': ['FIM_Communication_Score', 'FIM_Social_Cognition', 'FIM_Motor_Efficiency', 'FIM_Cognitive_Efficiency', 'Communication_per_Day', 'SocialCog_per_Day', 'FIM_CogMot_Ratio', 'FIM_CogMot_Product', 'FIM_CogItems_Avg']}\n"
     ]
    }
   ],
   "source": [
    "# ========================================================================\n",
    "# LOAD BEST MODEL FROM DISK\n",
    "# ========================================================================\n",
    "\n",
    "import joblib\n",
    "\n",
    "best_model_path = r\"C:\\Users\\saumy\\OneDrive\\Desktop\\tbi_pipeline_final_codes\\notebooks\\output\\models\\best_model_final.pkl\"\n",
    "best_model_instance = joblib.load(best_model_path)\n",
    "\n",
    "print(\"Best model loaded successfully!\")\n",
    "\n",
    "# ========================================================================\n",
    "# RUN RAW vs ENGINEERED FEATURE COMPARISON\n",
    "# ========================================================================\n",
    "\n",
    "filepath = \"../data/processed/df17nov.csv\"\n",
    "\n",
    "results = compare_raw_vs_engineered(\n",
    "    filepath=filepath,\n",
    "    best_model_pipeline=best_model_instance,\n",
    "    target_col='FIM_change',\n",
    "    test_size=0.2,\n",
    "    n_bootstrap=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Comparison Results:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25106c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
